{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DQN\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from lib import GetScreen, Actions, env, GetHp\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "cudnn.deterministic = True\n",
    "\n",
    "cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "import pygetwindow\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Experience = collections.namedtuple(\n",
    "    \"Experience\", field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"]\n",
    ")\n",
    "\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        state, action, reward, dones, next_state = zip(\n",
    "            *[self.buffer[idx] for idx in indices]\n",
    "        )\n",
    "        return (\n",
    "            torch.stack(state),\n",
    "            np.array(action),\n",
    "            np.array(reward, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool8),\n",
    "            torch.stack(next_state),\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DQN(7).to(device)\n",
    "tgt_net = DQN(7).to(device)\n",
    "# load the model\n",
    "try:\n",
    "    net.load_state_dict(torch.load(\"./checkpoints/best_model.pt\"))\n",
    "    frame_idx = np.load(\"./checkpoints/frame.npy\")\n",
    "    total_rewards = np.load(\"./checkpoints/total_rewards.npy\")\n",
    "    total_rewards=total_rewards.tolist()\n",
    "    tgt_net.load_state_dict(net.state_dict())\n",
    "    print(frame_idx)\n",
    "    print(\"load model\")\n",
    "except:\n",
    "    # if not, set epsilon\n",
    "    frame_idx = 0\n",
    "    total_rewards = []\n",
    "    print(\"new model\")\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.00025)\n",
    "\n",
    "writer = SummaryWriter(comment=\"deadcells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to load the before buffer\n",
    "try:\n",
    "    with open(\"./checkpoints/buffer.pickle\", \"rb\") as f:\n",
    "        buffer = pickle.load(f)\n",
    "    print(\"load buffer\")\n",
    "except:\n",
    "    buffer = ExperienceBuffer(capacity=10000)\n",
    "    print(\"new buffer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.hp=15482\n",
    "# boss.hp=215249\n",
    "# 15 7 7\n",
    "class Agent:\n",
    "    def __init__(self, exp_buffer):\n",
    "        self.buffer = exp_buffer\n",
    "        self.get_screen = GetScreen.GetScreen()\n",
    "        self.total_rewards = 0.0\n",
    "        self.env = env.env()\n",
    "        self.hpgetter = GetHp.Hp_getter()\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "        # self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.env._reset()\n",
    "        self.state = self.get_screen.grab()\n",
    "        self.total_rewards = 0.0\n",
    "        self.bosshp = self.hpgetter.get_boss_hp()\n",
    "        self.playerhp = self.hpgetter.get_self_hp()\n",
    "\n",
    "    def play_step(self, net, epsilon, device=\"cuda\"):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = random.randint(0, 6)\n",
    "\n",
    "        else:\n",
    "            q_val_v = net(self.state)\n",
    "            _, act_v = torch.max(q_val_v, dim=1)\n",
    "            action = int(act_v[0].item())\n",
    "\n",
    "        # Actions = [Attack,Shield, Roll, Jump, Move_Left, Move_Right, Nothing]\n",
    "        reward, is_done, self.playerhp, self.bosshp = self.env.step(\n",
    "            action, self.playerhp, self.bosshp\n",
    "        )\n",
    "\n",
    "        if reward != 0:\n",
    "            print(\"reward:%.2f,bosshp:%d,selfhp:%d \" %\n",
    "                  (reward, self.bosshp, self.playerhp), end='\\r')\n",
    "        new_state = self.get_screen.grab()\n",
    "        self.total_rewards += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.buffer.append(exp)\n",
    "\n",
    "        self.state = new_state\n",
    "\n",
    "        if is_done:\n",
    "            Actions.Nothing()\n",
    "            done_reward = self.total_rewards\n",
    "            # print(\"reward:%.2f\" % (done_reward), end='\\r')\n",
    "            time.sleep(11)\n",
    "        self.optimize_model()\n",
    "\n",
    "        tgt_net_dict = tgt_net.state_dict()\n",
    "        net_dict = net.state_dict()\n",
    "\n",
    "        for key in net_dict:\n",
    "            tgt_net_dict[key] = net_dict[key] * \\\n",
    "                TAU + tgt_net_dict[key]*(1-TAU)\n",
    "        tgt_net.load_state_dict(tgt_net_dict)\n",
    "        \n",
    "        return done_reward\n",
    "\n",
    "    def cal_loss(self, batch, net, tgt_net, device=\"cuda\"):\n",
    "        state, action, reward, done, next_state = batch\n",
    "\n",
    "        state_v = torch.squeeze(state, 1).to(device)\n",
    "        action_v = torch.tensor(action).to(device)\n",
    "        reward_v = torch.tensor(reward).to(device)\n",
    "        next_state_v = torch.squeeze(next_state, 1).to(device)\n",
    "\n",
    "        state_action_value = (\n",
    "            net(state_v).gather(\n",
    "                1, action_v.unsqueeze(-1).type(torch.long)).squeeze(-1)\n",
    "        )\n",
    "\n",
    "        next_state_value = tgt_net(next_state_v).max(1)[0]\n",
    "        next_state_value[done] = 0.0\n",
    "        next_state_value = next_state_value.detach()\n",
    "        expected_state_action = next_state_value * GAMMA + reward_v\n",
    "\n",
    "        return self.criterion(state_action_value, expected_state_action)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if buffer.__len__() < BATCH_SIZE:\n",
    "            return\n",
    "        batch = buffer.sample(BATCH_SIZE)\n",
    "        loss_t = self.cal_loss(batch, net, tgt_net)\n",
    "        writer.add_scalar(\"loss\", loss_t, frame_idx)\n",
    "        print(loss_t, end='\\r')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_t.backward()\n",
    "        torch.nn.utils.clip_grad_value_(net.parameters(), 100)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(buffer)\n",
    "best_mean_reward = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = pygetwindow.getWindowsWithTitle('Dead Cells')[0]\n",
    "win.size = (500, 500*1080/1920)\n",
    "agent.get_screen.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenbuffer:10000,frame:111604 game:118, mean reward: -265.529, eps:0.02\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# save buffer\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints/buffer.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_mean_reward \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest mean reward updated \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m, model saved\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;241m%\u001b[39m (best_mean_reward, mean_reward), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     55\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "done_reward = None\n",
    "\n",
    "\n",
    "# numbers of game\n",
    "time_start = time.time()\n",
    "agent._reset()\n",
    "\n",
    "while best_mean_reward is None or best_mean_reward > 50:\n",
    "    frame_idx += 1\n",
    "    epsilon = EPS_END + (EPS_START-EPS_END) * \\\n",
    "        math.exp(-1. * frame_idx / EPS_DECAY)\n",
    "\n",
    "    if done_reward is not None:\n",
    "        # one loop ends\n",
    "        # if something goes wrong, break\n",
    "\n",
    "        total_rewards.append(done_reward)\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "\n",
    "        print(\n",
    "            \"lenbuffer:%d,frame:%d game:%d, mean reward: %.3f, eps:%.2f\"\n",
    "            % (len(buffer), frame_idx, len(total_rewards), mean_reward, epsilon), end='\\r'\n",
    "        )\n",
    "        #     print(x, end='\\r')\n",
    "        # print()\n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "        writer.add_scalar(\"reward\", done_reward, frame_idx)\n",
    "\n",
    "        # save model\n",
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            torch.save(net.state_dict(), \"./checkpoints/best_model.pt\")\n",
    "            np.save(\"./checkpoints/frame.npy\", frame_idx)\n",
    "            # save buffer\n",
    "            with open(\"./checkpoints/buffer.pickle\", \"wb\") as f:\n",
    "                pickle.dump(copy.deepcopy(buffer), f)\n",
    "            if best_mean_reward is not None:\n",
    "                print(\n",
    "                    \"Best mean reward updated %.3f -> %.3f, model saved\"\n",
    "                    % (best_mean_reward, mean_reward), end='\\r'\n",
    "                )\n",
    "            best_mean_reward = mean_reward\n",
    "\n",
    "        # reset game\n",
    "        agent._reset()\n",
    "\n",
    "        # if for some random reason that agent do not enter the boss region\n",
    "        if not agent.hpgetter.get_boss_hp():\n",
    "            time.sleep(1)\n",
    "            Actions.Move_Right()\n",
    "            time.sleep(8)\n",
    "            Actions.Nothing()\n",
    "            Actions.Move_Left()\n",
    "            time.sleep(5.5)\n",
    "            Actions.Nothing()\n",
    "            agent._reset()\n",
    "    # play a step\n",
    "    done_reward = agent.play_step(net, epsilon, device)\n",
    "\n",
    "\n",
    "writer.close()\n",
    "print(time.localtime())\n",
    "print((time.time()-time_start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"./checkpoints/best_model.pt\")\n",
    "np.save(\"./checkpoints/frame.npy\", frame_idx)\n",
    "np.save(\"./checkpoints/total_rewards.npy\", total_rewards)\n",
    "with open(\"./checkpoints/buffer.pickle\", \"wb\") as f:\n",
    "    pickle.dump(copy.deepcopy(buffer), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
